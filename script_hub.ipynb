{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "script_hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ml2-picme/PicMe/blob/master/script_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDKd_blagdCz",
        "colab_type": "text"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYeihUkTgwSO",
        "colab_type": "code",
        "outputId": "1310644a-51f7-4c76-c0a3-b29c2713e996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Imports used for this notebook\n",
        "\n",
        "import sys\n",
        "import hashlib\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from keras.applications import *\n",
        "\n",
        "# PIP installations\n",
        "\n",
        "!pip install mysql-connector-python-rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mysql-connector-python-rf in /usr/local/lib/python3.6/dist-packages (2.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkZ7mK_cgcse",
        "colab_type": "code",
        "outputId": "556259ff-dd02-4d5f-d8d2-a2960337a2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Clone the actual project into the Colab instance, for enabling the usage of the self-written scripts\n",
        "\n",
        "# Remove Path if already existing locally\n",
        "!rm -r PicMe\n",
        "\n",
        "# Clone Git repository\n",
        "!git clone https://github.com/ml2-picme/PicMe.git\n",
        "\n",
        "# Add the relevant paths of the repo to system path\n",
        "sys.path.append(\"/content/PicMe\")\n",
        "sys.path.append(\"/content/PicMe/scripts\")\n",
        "\n",
        "# Add the functions, defined in the script files\n",
        "\n",
        "import db_connector\n",
        "import file_processing\n",
        "import image_classification\n",
        "import imagenet_tree_search\n",
        "import text_processing\n",
        "import email_processing"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PicMe'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 543 (delta 12), reused 0 (delta 0), pack-reused 515\n",
            "Receiving objects: 100% (543/543), 13.33 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (296/296), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jScO3LyUeBUL",
        "colab_type": "text"
      },
      "source": [
        "### Entry methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64s3IgXPe29u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This method creates the re-used DB connection (used to save and read search results)\n",
        "# TODO Add parameters to createConnection method\n",
        "\n",
        "def createDbConnection(dbUser, dbPassword, dbHost, dbDatabase, dbAutoCommit):\n",
        "  return db_connector.createConnection(dbUser, dbPassword, dbHost, dbDatabase, dbAutoCommit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dkUwvLcdo5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the entry method for directory preparation. It downloads the files from ./input directory into local directories on colab instance\n",
        "\n",
        "def simulateDirectoryStructure(path, hashrange):\n",
        "  createDirectoryStructure(path, hashrange)\n",
        "  downloadPictures(path, hashrange)\n",
        "  downloadEmails(path, hashrange)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7a7-3BDeSSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the entry method for examining the local directory structure. It looks for images, emails and documents and writes results to DB\n",
        "\n",
        "def examineDirectoryStructure(path, imageExtensions, emailExtensions, documentExtensions, dbConnection):\n",
        "  examineImages(path, imageExtensions, dbConnection)\n",
        "  examineEmails(path, emailExtensions, dbConnection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SxyVv9bi0HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the entry method for finding results based on a fix search-term\n",
        "\n",
        "def searchByTerm(searchTerm, dbConnection):\n",
        "  searchTheDbBasedOnTerm(searchTerm, dbConnection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtQRT5iHi9oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the entry method for automatically finding image-text mappings\n",
        "\n",
        "def searchImageTextMappings(dbConnection):\n",
        "  searchDbAutomaticallyForImageTextMappings(dbConnection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVO5DM8Slsoy",
        "colab_type": "text"
      },
      "source": [
        "### Method definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvE9oN8Lm6dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createDirectoryStructure(path, hashrange):\n",
        " \n",
        "  # Step 1: Delete local files, if existing\n",
        "  file_processing.deleteLocalDirectory(path)\n",
        "  \n",
        "  # Step 2: Re-create local directory structure\n",
        "  for i in range(hashrange):\n",
        "    if(i % 10 == 0):\n",
        "      parentPath = path + \"/\" + str((int)(i/10))\n",
        "      file_processing.createLocalDirectory(parentPath)\n",
        "    normalizedI = '%02d' % i  # Normalization, pad zeroes\n",
        "    filePath = parentPath + \"/\" + normalizedI\n",
        "    file_processing.createLocalDirectory(filePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Xt5MDklz2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downloadPictures(path, hashrange):\n",
        "  \n",
        "  filesDict = {}\n",
        "\n",
        "  data = urlopen(\"https://raw.githubusercontent.com/ml2-picme/PicMe/master/input/images_subset.txt\")\n",
        "  for line in data:\n",
        "    if not line.startswith(b'#'):                     # Ignore Lines that begin with a comment (#)\n",
        "      line = line.decode(\"utf-8\").split(\"\\n\")[0]      # Normalization\n",
        "      url = line.split(\";\")[0]\n",
        "      label = line.split(\";\")[1]\n",
        "\n",
        "      filename = file_processing.getFileNameFromPath(url)\n",
        "\n",
        "      hashvalue = int(hashlib.sha1(filename.encode('utf-8')).hexdigest(), 16) % hashrange  # get the hash-value from filename\n",
        "      parent_dir = (int)(hashvalue / 10)\n",
        "      hashvalue = '%02d' % hashvalue                  # Normalization, pad zeroes\n",
        "\n",
        "      filetype = filename.split(\".\")[len(filename.split(\".\")) - 1]\n",
        "      newFilename = label + \".\" + filetype\n",
        "\n",
        "      print(url, \" -> \", hashvalue, \" -> \", label, \" -> \", parent_dir, \" -> \", filename)\n",
        "\n",
        "      localPath = path + \"/\" + str(parent_dir) + \"/\" + hashvalue + \"/\" + newFilename\n",
        "      file_processing.downloadFileFromUrl(url, localPath)\n",
        "      filesDict[localPath] = url\n",
        "      \n",
        "  return filesDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZc6FLL5l2fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downloadEmails(path, hashrange):\n",
        "  filesDict = {}\n",
        "\n",
        "  data = urlopen(\"https://raw.githubusercontent.com/ml2-picme/PicMe/master/input/emails.txt\")\n",
        "  for line in data:\n",
        "    if not line.startswith(b'#'):                     # Ignore Lines that begin with a comment (#)\n",
        "      url = line.decode(\"utf-8\").split(\"\\n\")[0]      # Normalization\n",
        "\n",
        "      filename = file_processing.getFileNameFromPath(url)\n",
        "\n",
        "      hashvalue = int(hashlib.sha1(filename.encode('utf-8')).hexdigest(), 16) % hashrange  # get the hash-value from filename\n",
        "      parent_dir = (int)(hashvalue / 10)\n",
        "      hashvalue = '%02d' % hashvalue                  # Normalization, pad zeroes\n",
        "\n",
        "      print(url, \" -> \", hashvalue, \" -> \", parent_dir, \" -> \", filename)\n",
        "\n",
        "      localPath = path + \"/\" + str(parent_dir) + \"/\" + hashvalue + \"/\" + filename\n",
        "      file_processing.downloadFileFromUrl(url, localPath)\n",
        "      filesDict[localPath] = url\n",
        "      \n",
        "  return filesDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ubf88xKl4hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def examineImages(path, imageExtensions, dbConnection):\n",
        "  \n",
        "  # Step 1: Search the directory based on file extensions\n",
        "  foundFiles = file_processing.findFilesInPathByFileExtension(path, imageExtensions)\n",
        "\n",
        "  for foundFile in foundFiles:\n",
        "    print(foundFile)\n",
        "\n",
        "  # Step 2: Prepare the found images for classification\n",
        "  preparedImages224x224 = image_classification.prepareImagesForClassification(foundFiles, 224, 224)\n",
        "  preparedImages299x299 = image_classification.prepareImagesForClassification(foundFiles, 299, 299)\n",
        "  \n",
        "  # Step 3: CLASSIFY THE IMAGES\n",
        "  # Important: We give functions here: \n",
        "  # 1) preprocess_input function\n",
        "  # 2) decode_predictions function\n",
        "  # => These functions are model-dependent!\n",
        "  predictedClassesVGG16 = image_classification.classifyImages(preparedImages224x224, vgg16.preprocess_input, vgg16.decode_predictions, vgg16.VGG16(input_shape=(224, 224, 3)))\n",
        "  predictedClassesVGG19 = image_classification.classifyImages(preparedImages224x224, vgg19.preprocess_input, vgg19.decode_predictions, vgg19.VGG19(input_shape=(224, 224, 3)))\n",
        "  #predictedClassesMobileNetV2 = classifyImages(preparedImages224x224, mobilenet_v2.preprocess_input, mobilenet_v2.decode_predictions, mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3)))\n",
        "  #predictedClassesResNet50 = classifyImages(preparedImages224x224, resnet50.preprocess_input, resnet50.decode_predictions, resnet50.ResNet50(input_shape=(224, 224, 3)))\n",
        "  #predictedClassesDenseNet201 = classifyImages(preparedImages224x224, densenet.preprocess_input, densenet.decode_predictions, densenet.DenseNet201(input_shape=(224, 224, 3)))\n",
        "  #predictedClassesInceptionV3 = classifyImages(preparedImages299x299, inception_v3.preprocess_input, inception_v3.decode_predictions, inception_v3.InceptionV3(input_shape=(299, 299, 3)))\n",
        "  #predictedClassesXception = classifyImages(preparedImages299x299, xception.preprocess_input, xception.decode_predictions, xception.Xception(input_shape=(299, 299, 3)))\n",
        "  #predictedClassesInceptionResNet = classifyImages(preparedImages299x299, inception_resnet_v2.preprocess_input, inception_resnet_v2.decode_predictions,inception_resnet_v2.InceptionResNetV2(input_shape=(299, 299, 3)))\n",
        "  \n",
        "  resultsList = [predictedClassesVGG16, predictedClassesVGG19]#, predictedClassesMobileNetV2, predictedClassesResNet50, predictedClassesDenseNet201, predictedClassesInceptionV3, predictedClassesXception, predictedClassesInceptionResNet]\n",
        "  modelList = ['VGG16', 'VGG19']#, 'MobileNetV2', 'ResNet50', 'DenseNet201', 'InceptionV3', 'Xception', 'InceptionResNet']\n",
        "  \n",
        "  # Step 4: Iterate the results\n",
        "  # a) save the result to DB\n",
        "  # b) search the ImageNet tree to expand the list of matching class names\n",
        "  # c) save also these results to DB\n",
        "  \n",
        "  parentToChildrenDictionary = imagenet_tree_search.getParentToChildrenDictionary()\n",
        "  childToParentsDictionary = imagenet_tree_search.getChildToParentsDictionary()\n",
        "  \n",
        "  # Iterating the classification results:\n",
        "  for k in range(len(modelList)):\n",
        "      print(\"==== other model =====\")\n",
        "      for i in range(len(foundFiles)):\n",
        "        print(\"==== other file =====\")\n",
        "        for j in range(5):\n",
        "          print(\"Counter:\")\n",
        "          print(\"Model\", (k+1), \"of\", len(modelList))\n",
        "          print(\"File\", (i+1), \"of\", len(foundFiles))\n",
        "          print(\"Top\", (j+1), \"of\", 5)\n",
        "\n",
        "          fileName = foundFiles[i]\n",
        "          modelName = modelList[k]\n",
        "          predictedClassSynsetId = resultsList[k][i][j][0]\n",
        "          predictedClass = resultsList[k][i][j][1]\n",
        "          predictedProbability = resultsList[k][i][j][2]\n",
        "\n",
        "          # a) Store the original class to DB\n",
        "          db_connector.storeImageClassificationResultToDB(dbConnection, fileName, modelName, predictedClass, predictedProbability)\n",
        "\n",
        "          # b) Expand ImageNet classes by ImageNet tree search\n",
        "          newWords = imagenet_tree_search.getWords(predictedClassSynsetId, parentToChildrenDictionary, childToParentsDictionary)\n",
        "\n",
        "          # c) Also save these new results to DB\n",
        "          for newWord in newWords:\n",
        "            db_connector.storeImageClassificationResultToDB(dbConnection, fileName, modelName, newWord, predictedProbability)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw6gGxMWl8uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def examineEmails(path, emailExtensions, dbConnection):\n",
        "  \n",
        "  emailsDict = {}\n",
        "  stemmingStopWords = text_processing.prepare()\n",
        "  \n",
        "  # Step 1: Search the directory based on file extensions\n",
        "  emailList = file_processing.findFilesInPath(localEmailPath, extensionsToCheck)\n",
        "  \n",
        "  # Step 2: Iterate the Emails and examine their content\n",
        "  for email in emailList:\n",
        "    \n",
        "    # a) Examine email content\n",
        "    emailDict = examineEmail(email)\n",
        "    \n",
        "    emailPath = email\n",
        "    emailFrom = emailDict[\"from\"]\n",
        "    emailTo = emailDict[\"to\"]\n",
        "    emailSubject = emailDict[\"subject\"]\n",
        "    emailBody = emailDict[\"body\"]\n",
        "    \n",
        "    # b) Prepare for stemming\n",
        "    normalizedSubject = text_processing.normalizeWords(emailSubject, stemmingStopWords)\n",
        "    normalizedBody = text_processing.normalizeWords(emailBody, stemmingStopWords)\n",
        "    \n",
        "    stemmingWords = []\n",
        "    \n",
        "    stemmedBody = text_processing.stem(normalizedSubject)\n",
        "    stemmedSubject = text_processing.stem(normalizedBody)\n",
        "    \n",
        "    stemmingWords.extend(stemmedSubject)\n",
        "    stemmingWords.extend(stemmedBody)\n",
        "    \n",
        "    # c) Store the results into DB\n",
        "    storeTextStemmingResultToDB(dbConnection, emailPath, emailFrom, emailTo, emailSubject, emailBody, stemmingWords)\n",
        "      \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOGCPHKljAoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e40f1c33-03d4-4a42-b34d-1fd37b7525d3"
      },
      "source": [
        "# TODO Remove\n",
        "# b) Prepare for stemming\n",
        "\n",
        "stopWords = text_processing.prepare()\n",
        "\n",
        "\n",
        "normalizedSubject = text_processing.normalizeWords(\"This is a subject\", stopWords)\n",
        "normalizedBody = text_processing.normalizeWords(\"This is a test body\", stopWords)\n",
        "    \n",
        "stemmedMail = []\n",
        "    \n",
        "stemmedBody = text_processing.stem(normalizedSubject)\n",
        "stemmedSubject = text_processing.stem(normalizedBody)\n",
        "    \n",
        "stemmedMail.extend(stemmedSubject)\n",
        "stemmedMail.extend(stemmedBody)\n",
        "\n",
        "print(stemmedMail)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]   Package porter_test is already up-to-date!\n",
            "['test', 'bodi', 'subject']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCK2C7Dll-kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def searchTheDbBasedOnTerm(searchTerm, dbConnection):\n",
        "  resultCursor = db_connector.querySearchWordAndPrintResults(dbConnection, searchTerm, image_classification.prepareImagesForClassification)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZeS2ZitmK2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def searchDbAutomaticallyForImageTextMappings(dbConnection):\n",
        "  print(\"todo\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}